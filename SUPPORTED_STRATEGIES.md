# VLMEvalKit Supported Strategies Analysis

This document analyzes VLMEvalKit against the Unified Evaluation Workflow to determine which strategies are supported by the harness.

## Methodology

Strategies are classified into three categories:

### **Natively Supported**
Steps that meet ALL of the following requirements:
- Available immediately after installing the evaluation harness (`pip install -e .`)
- Requires only import statements and minimal configuration (‚â§2 lines)
- No external dependencies beyond the harness's standard installation
- No custom implementation or glue code required

### **Supported via Third-Party Integration**
Steps that meet ALL of the following requirements:
- Requires installing ‚â•1 external package(s) beyond the harness
- Requires glue code or configuration (typically ‚â§10 lines)
- Has documented integration pattern or official example in the harness documentation
- Functionality enabled through third-party tools rather than the harness alone

### **Not Supported**
Steps that do not meet either of the above criteria:
- No documented integration pattern
- Requires extensive custom implementation
- Not available through any documented means

---

## Phase 0: Provisioning (The Runtime)

### Step A: Harness Installation

#### ‚úÖ Strategy 1: PyPI Packages
**STATUS: Natively Supported**

VLMEvalKit can be installed via pip:
```bash
pip install -e .
```
The `setup.py` file defines the package as `vlmeval` with all dependencies listed in `requirements.txt`.

**Evidence:**
- `setup.py` contains standard Python package configuration
- Installation via `pip install -e .` documented in QuickStart
- Package name: `vlmeval`

#### ‚úÖ Strategy 2: Git Clone
**STATUS: Natively Supported**

VLMEvalKit supports installation from source via git clone:
```bash
git clone https://github.com/open-compass/VLMEvalKit.git
cd VLMEvalKit
pip install -e .
```

**Evidence:**
- Official documentation shows git clone as primary installation method
- `docs/en/Quickstart.md` line 11-14

#### ‚ùå Strategy 3: Container Images
**STATUS: Not Supported**

VLMEvalKit does not provide prebuilt Docker or OCI container images in its standard distribution.

**Evidence:**
- No Dockerfile found in repository root
- No container image documentation in QuickStart or README
- No references to Docker Hub or container registries

#### ‚ùå Strategy 4: Binary Packages
**STATUS: Not Supported**

VLMEvalKit does not provide standalone executable binaries.

**Evidence:**
- Python-based toolkit with no compiled binary distribution
- Installation requires Python environment

#### ‚ùå Strategy 5: Node Package
**STATUS: Not Supported**

VLMEvalKit is a Python-based toolkit, not a Node.js package.

**Evidence:**
- No `package.json` found
- Pure Python implementation

### Step B: Service Authentication

#### ‚ùå Strategy 1: Evaluation Platform Authentication
**STATUS: Not Supported**

While VLMEvalKit can submit results to leaderboards (OpenCompass, HuggingFace), it does not provide native command-line authentication flows for evaluation platform services.

**Evidence:**
- README mentions leaderboards (line 9, 14, 16) but no authentication API
- No CLI authentication commands documented
- Results are displayed/downloaded but no programmatic submission API exposed

#### ‚úÖ Strategy 2: API Provider Authentication
**STATUS: Natively Supported**

VLMEvalKit supports API key configuration for commercial model providers via environment variables or `.env` file.

**Evidence:**
- `docs/en/Quickstart.md` lines 20-50 show API key configuration
- Supports: OPENAI_API_KEY, DASHSCOPE_API_KEY, GOOGLE_API_KEY, GEMINI_API_KEY, etc.
- `.env` file support for key management

#### ‚úÖ Strategy 3: Repository Authentication
**STATUS: Natively Supported**

VLMEvalKit uses HuggingFace Hub for model and dataset access, supporting token-based authentication.

**Evidence:**
- Uses `huggingface_hub` package (requirements.txt line 8)
- Downloads models from HuggingFace using standard authentication
- Dataset downloads from HuggingFace repositories

---

## Phase I: Specification (The Contract)

### Step A: SUT Preparation

#### ‚úÖ Strategy 1: Model-as-a-Service (Remote Inference)
**STATUS: Natively Supported**

VLMEvalKit supports API-based models through the `vlmeval/api/` module.

**Evidence:**
- `vlmeval/api/` directory contains 20+ API wrappers:
  - `gpt.py` - OpenAI models
  - `gemini.py` - Google Gemini
  - `qwen_api.py`, `qwen_vl_api.py` - Qwen APIs
  - `claude.py` - Anthropic Claude
  - `hunyuan.py`, `doubao_vl_api.py`, etc.
- `config.py` contains API model configurations
- QuickStart documents API key setup for remote inference

#### ‚úÖ Strategy 2: Model-in-Process (Local Inference)
**STATUS: Natively Supported**

VLMEvalKit supports local model loading and inference through the `vlmeval/vlm/` module.

**Evidence:**
- `vlmeval/vlm/` directory contains 200+ model implementations
- Supports loading from HuggingFace transformers
- Local weight loading via `transformers` library
- README shows local model evaluation examples (line 88-97)
- Transformer version recommendations for different models (line 63-74)

#### ‚ùå Strategy 3: Algorithm Implementation (In-Memory Structures)
**STATUS: Not Supported**

VLMEvalKit focuses on vision-language models and does not provide native support for ANN algorithms, knowledge graph embeddings, or specialized indexes like FAISS/HNSW.

**Evidence:**
- No evidence of ANN algorithm support in codebase
- Focus is exclusively on VLM evaluation
- No vector index or knowledge graph implementations

#### ‚ùå Strategy 4: Policy/Agent Instantiation (Stateful Controllers)
**STATUS: Not Supported**

VLMEvalKit does not provide native support for RL policies, autonomous agents, or multi-agent systems.

**Evidence:**
- No RL policy or agent framework in vlmeval/
- Focus is on VLM generation, not sequential decision-making
- No environment interaction loops for agents

### Step B: Benchmark Preparation (Inputs)

#### ‚úÖ Strategy 1: Benchmark Dataset Preparation (Offline)
**STATUS: Natively Supported**

VLMEvalKit provides extensive benchmark dataset support with automatic downloading and preprocessing.

**Evidence:**
- 333 dataset-related Python files in `vlmeval/dataset/`
- Automatic TSV download from `DATASET_URL` (Development.md line 18)
- Support for 70+ image and video benchmarks (README line 59)
- Dataset preprocessing in `ImageBaseDataset` class
- Automatic data splitting and formatting

#### ‚ùå Strategy 2: Synthetic Data Generation (Generative)
**STATUS: Not Supported**

VLMEvalKit does not provide native test data generation, perturbation, or augmentation capabilities out-of-the-box.

**Evidence:**
- No data generation modules found
- Focus is on pre-existing benchmark evaluation
- No synthetic data creation documented

#### ‚ùå Strategy 3: Simulation Environment Setup (Simulated)
**STATUS: Not Supported**

VLMEvalKit does not provide simulation environments for interactive tasks.

**Evidence:**
- No 3D environment simulation
- No physics simulation or scene construction
- Focus is on static image/video benchmarks

#### ‚ùå Strategy 4: Production Traffic Sampling (Online)
**STATUS: Not Supported**

VLMEvalKit does not support real-world traffic sampling or streaming inference.

**Evidence:**
- No traffic sampling functionality
- Batch evaluation only
- No online/streaming mode documented

### Step C: Benchmark Preparation (References)

#### ‚úÖ Strategy 1: Judge Preparation
**STATUS: Natively Supported**

VLMEvalKit supports LLM-based judges for evaluation through OpenAI API or local LLM deployment.

**Evidence:**
- QuickStart line 19: "use LLM APIs as the judge or choice extractor"
- Support for GPT-based judging: `chatgpt-0125`, `gpt-4-0125` (image_mcq.py line 158)
- Local LLM judge deployment via LMDeploy (Quickstart.md line 177-216)
- Judge configuration via `judge_kwargs` parameter

#### ‚úÖ Strategy 2: Ground Truth Preparation
**STATUS: Natively Supported**

VLMEvalKit pre-loads ground truth from benchmark TSV files including human annotations and reference answers.

**Evidence:**
- TSV format includes 'answer' field (Development.md line 47)
- Ground truth loaded in `ImageBaseDataset.__init__` (image_base.py)
- Benchmark datasets include reference answers for evaluation

---

## Phase II: Execution (The Run)

### Step A: SUT Invocation

#### ‚úÖ Strategy 1: Batch Inference
**STATUS: Natively Supported** (with optional third-party acceleration)

VLMEvalKit performs batch inference across benchmark datasets with fixed model instances.

**Evidence:**
- Main evaluation via `run.py` processes entire datasets
- `infer_data_job` function in inference.py handles batch processing
- Support for parallel batch inference via torchrun (Quickstart.md line 73-90)
- Multi-GPU batch inference documented

**Optional Third-Party Acceleration:**
- LMDeploy integration for faster inference (requires `pip install lmdeploy`)
  - Documented in `docs/en/EvalByLMDeploy.md`
  - README line 39: "supports multi-node distributed inference using LMDeploy"
- vLLM integration (requires `pip install vllm`)
  - Enabled via `use_vllm` flag in model configuration

#### ‚ùå Strategy 2: Interactive Loop
**STATUS: Not Supported**

While VLMEvalKit supports multi-turn conversations through `chat_inner` API (Development.md line 111-127), it does not provide native stateful environment stepping or physics simulation loops for interactive evaluation.

**Evidence:**
- `chat_inner` API exists for conversation but not environment interaction
- No environment step() interfaces
- No interactive simulation loops

#### ‚ùå Strategy 3: Arena Battle
**STATUS: Not Supported**

VLMEvalKit does not provide native pairwise model comparison or arena battle functionality.

**Evidence:**
- No arena battle mode in run.py
- No pairwise comparison functionality documented
- Evaluates models independently, not head-to-head

#### ‚ùå Strategy 4: Production Streaming
**STATUS: Not Supported**

VLMEvalKit does not support real-time production traffic processing.

**Evidence:**
- Batch-only evaluation mode
- No streaming inference capability
- No real-time metric collection

---

## Phase III: Assessment (The Score)

### Step A: Individual Scoring

#### ‚úÖ Strategy 1: Deterministic Measurement
**STATUS: Natively Supported**

VLMEvalKit supports exact matching and deterministic evaluation metrics.

**Evidence:**
- Exact matching mode: `model = 'exact_matching'` (image_mcq.py line 158)
- QuickStart line 19: "exact matching mode (find 'Yes', 'No', 'A', 'B', 'C'... in the output strings)"
- Answer extraction without LLM for MCQ and Yes/No tasks
- Matching utilities in `vlmeval/utils/matching_util.py`

#### üîß Strategy 2: Embedding Measurement
**STATUS: Supported via Third-Party Integration**

VLMEvalKit supports embedding-based metrics through optional dataset-specific packages.

**Evidence:**
- ROUGE metric in image_caption.py line 12: `(Rouge(), 'ROUGE_L')`
  - Requires `pycocoevalcap` package (not in main requirements.txt)
  - Imported from `pycocoevalcap.rouge.rouge` for COCO captioning benchmarks
- BERTScore support in uni_svg.py
  - Requires `pip install bert-score` (optional dependency)
  - Imported from `bert_score import BERTScorer`
- Dataset-specific requirements files provide these dependencies when needed
  - Example: `vlmeval/dataset/OmniDocBench/requirements.txt` includes evaluation packages

**Integration Pattern:**
```python
# Requires: pip install bert-score
from bert_score import BERTScorer
bert_scorer = BERTScorer(lang="en", rescale_with_baseline=True)
```

#### ‚úÖ Strategy 3: Subjective Measurement
**STATUS: Natively Supported**

VLMEvalKit supports LLM-based judgments for subjective evaluation.

**Evidence:**
- LLM judge support via OpenAI API (Quickstart.md line 19)
- GPT-based answer extraction and evaluation
- Local LLM judge deployment option (Quickstart.md line 177-216)
- Choice extractor using LLMs for subjective assessment

#### ‚ùå Strategy 4: Performance Measurement
**STATUS: Not Supported**

VLMEvalKit does not provide native latency, throughput, or resource consumption measurement.

**Evidence:**
- No performance profiling documented
- No latency/throughput metrics in evaluation outputs
- Focus is on accuracy, not efficiency metrics
- `get_gpu_memory()` utility exists but not used for formal benchmarking

### Step B: Collective Aggregation

#### ‚úÖ Strategy 1: Score Aggregation
**STATUS: Natively Supported**

VLMEvalKit aggregates individual scores into benchmark-level metrics.

**Evidence:**
- All dataset classes implement `evaluate()` method returning aggregate metrics
- Results output as CSV files with aggregate scores (Quickstart.md line 105)
- Development.md line 55-57: evaluate function outputs aggregate results as dict/DataFrame
- Per-category and overall accuracy computation

#### ‚ùå Strategy 2: Uncertainty Quantification
**STATUS: Not Supported**

VLMEvalKit does not provide bootstrap resampling or confidence interval computation out-of-the-box.

**Evidence:**
- No bootstrap or uncertainty quantification code found
- No confidence interval computation in evaluation outputs
- Only point estimates reported

---

## Phase IV: Reporting (The Output)

### Step A: Insight Presentation

#### ‚ùå Strategy 1: Execution Tracing
**STATUS: Not Supported**

VLMEvalKit does not provide detailed step-by-step execution tracing of model reasoning paths.

**Evidence:**
- No execution trace visualization
- Logs show progress but not detailed reasoning steps
- No tool call or decision path tracking

#### ‚úÖ Strategy 2: Subgroup Analysis
**STATUS: Natively Supported**

VLMEvalKit supports breaking down results by categories and domains.

**Evidence:**
- Benchmark TSV files include 'category', 'l2-category' fields (Development.md Table 1)
- VideoMME evaluation includes domain-specific results (videomme.py)
- Per-category accuracy breakdown in evaluation outputs
- Stratification by task categories in benchmark results

#### ‚ùå Strategy 3: Chart Generation
**STATUS: Not Supported**

VLMEvalKit does not provide native chart or visualization generation capabilities.

**Evidence:**
- Results output as CSV/Excel files, not charts
- No plotting/visualization code in vlmeval/
- Matplotlib imported in requirements.txt but not used for output generation

#### ‚ùå Strategy 4: Dashboard Creation
**STATUS: Not Supported**

VLMEvalKit does not create interactive web dashboards for results display.

**Evidence:**
- No web dashboard implementation
- Gradio imported for demos but not for results dashboards
- No interactive result exploration UI

#### ‚úÖ Strategy 5: Leaderboard Publication
**STATUS: Natively Supported** (compatibility only, not automated submission)

VLMEvalKit supports leaderboard integration through data compatibility with OpenCompass and HuggingFace leaderboards.

**Evidence:**
- README line 9: "OC Learderboard" link
- README line 14: "HF Leaderboard" link  
- README line 57: "performance numbers on our official multi-modal leaderboards can be downloaded"
- Results format compatible with leaderboard submission
- Note: Actual submission requires manual process, not automated API

#### ‚ùå Strategy 6: Regression Alerting
**STATUS: Not Supported**

VLMEvalKit does not provide automated regression detection or alerting.

**Evidence:**
- No baseline comparison functionality
- No alerting system
- No performance degradation detection

---

## Summary Statistics

### Supported Strategies by Phase and Type

**Phase 0: Provisioning**
- Natively Supported: 4/8 (50%)
  - Harness Installation: 2/5 (PyPI, Git Clone)
  - Service Authentication: 2/3 (API Provider, Repository)
- Third-Party Integration: 0/8
- Not Supported: 4/8 (Container Images, Binary Packages, Node Package, Platform Auth)

**Phase I: Specification**
- Natively Supported: 5/10 (50%)
  - SUT Preparation: 2/4 (Model-as-a-Service, Model-in-Process)
  - Benchmark Preparation (Inputs): 1/4 (Offline Datasets)
  - Benchmark Preparation (References): 2/2 (Judge, Ground Truth)
- Third-Party Integration: 0/10
- Not Supported: 5/10 (Algorithms, Agents, Synthetic Data, Simulations, Production Traffic)

**Phase II: Execution**
- Natively Supported: 1/4 (25%)
  - SUT Invocation: 1/4 (Batch Inference - native, with optional third-party acceleration)
- Third-Party Integration: 0/4
- Not Supported: 3/4 (Interactive Loops, Arena Battles, Production Streaming)

**Phase III: Assessment**
- Natively Supported: 3/6 (50%)
  - Individual Scoring: 2/4 (Deterministic, Subjective)
  - Collective Aggregation: 1/2 (Score Aggregation)
- Third-Party Integration: 1/6 (17%)
  - Individual Scoring: 1/4 (Embedding Measurement via bert-score, pycocoevalcap)
- Not Supported: 2/6 (Performance Measurement, Uncertainty Quantification)

**Phase IV: Reporting**
- Natively Supported: 2/6 (33%)
  - Insight Presentation: 2/6 (Subgroup Analysis, Leaderboard Compatibility)
- Third-Party Integration: 0/6
- Not Supported: 4/6 (Execution Tracing, Charts, Dashboards, Regression Alerting)

### Overall Support
- **Natively Supported: 15/34 strategies (44%)**
- **Third-Party Integration: 1/34 strategies (3%)**
- **Total Supported (Native + Integration): 16/34 strategies (47%)**
- **Not Supported: 18/34 strategies (53%)**

### Core Strengths
VLMEvalKit excels at:
1. ‚úÖ **Natively Supported:** Vision-Language Model evaluation (API and local)
2. ‚úÖ **Natively Supported:** Benchmark dataset management and preprocessing
3. ‚úÖ **Natively Supported:** LLM-based and deterministic evaluation metrics
4. ‚úÖ **Natively Supported:** Multi-GPU batch inference
5. ‚úÖ **Natively Supported:** Subgroup analysis by categories
6. ‚úÖ **Natively Supported:** API provider authentication
7. üîß **Third-Party Integration:** Embedding-based metrics (ROUGE, BERTScore)
8. üîß **Optional Acceleration:** LMDeploy and vLLM for faster batch inference

### Key Gaps
VLMEvalKit does not support:
1. ‚ùå Container deployment
2. ‚ùå Synthetic data generation
3. ‚ùå Interactive environment simulation
4. ‚ùå Production traffic streaming
5. ‚ùå Arena battles / pairwise comparison
6. ‚ùå Performance profiling (latency/throughput)
7. ‚ùå Uncertainty quantification
8. ‚ùå Execution tracing
9. ‚ùå Chart/dashboard generation
10. ‚ùå Automated regression alerting

---

## Conclusion

VLMEvalKit is a specialized evaluation harness focused on **offline batch evaluation of vision-language models** on **pre-existing benchmarks** with support for both **API-based and local inference**. 

### Native Capabilities (15/34 strategies - 44%)
The harness provides strong native support for:
- Dataset management and preprocessing
- Multiple evaluation paradigms (deterministic, LLM-judged)
- Category-level result breakdown
- Multi-GPU distributed inference
- API and repository authentication

### Third-Party Integrations (1/34 strategies - 3%)
With documented integration patterns:
- **Embedding-based metrics** via bert-score and pycocoevalcap packages
- **Optional acceleration** via LMDeploy and vLLM (documented but not counted as separate strategies)

### Not Supported (18/34 strategies - 53%)
The harness is **not** a general-purpose evaluation framework for:
- Interactive/sequential decision-making (RL, agents)
- Production deployment monitoring
- Real-time streaming evaluation
- Synthetic data generation
- Performance/efficiency benchmarking
- Automated visualization and dashboarding

### Use Case Fit
VLMEvalKit is well-suited for researchers and developers who need to:
- Evaluate VLMs on standard academic benchmarks
- Use flexible inference backends (API, local, or accelerated via LMDeploy/vLLM)
- Perform category-level analysis of model performance
- Leverage both deterministic and LLM-based evaluation

The harness would require significant custom development to support advanced evaluation workflows like:
- Arena battles or pairwise model comparison
- Uncertainty quantification with confidence intervals
- Production monitoring with regression detection
- Interactive agent evaluation in simulated environments
